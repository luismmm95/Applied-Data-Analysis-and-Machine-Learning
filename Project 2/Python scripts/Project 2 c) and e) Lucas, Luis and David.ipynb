{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 c) and e) solution David, Lucas and Luis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Importing implemented Neural Network functions\n",
    "import mlp\n",
    "\n",
    "import warnings\n",
    "#Comment this to turn on warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from random import randrange\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data (may lead to memory error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ae69843e1c1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpackbits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1600\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Decompress array and reshape for convenience\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;31m# map 0 state to -1 (Ising variable can take values +/-1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Ising2DFM_reSample_L40_T=All_labels.pkl\"\u001b[0m \u001b[1;31m# this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### prepare training and test data sets\n",
    "\n",
    "import pickle,os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###### define ML parameters\n",
    "num_classes=2\n",
    "train_to_test_ratio=0.5 # training samples\n",
    "\n",
    "# load data\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
    "data = pickle.load(open(file_name,'rb')) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
    "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "data = data.astype('int')\n",
    "data[np.where(data == 0)] =-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\" # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
    "labels = pickle.load(open(file_name,'rb')) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
    "\n",
    "# divide data into ordered, critical and disordered\n",
    "X_ordered=data[:70000,:]\n",
    "Y_ordered=labels[:70000]\n",
    "\n",
    "#X_critical=data[70000:100000,:]\n",
    "#Y_critical=labels[70000:100000]\n",
    "\n",
    "X_disordered=data[100000:,:]\n",
    "Y_disordered=labels[100000:]\n",
    "\n",
    "del data,labels\n",
    "\n",
    "# define training and test data sets\n",
    "X=np.concatenate((X_ordered,X_disordered))\n",
    "Y=np.concatenate((Y_ordered,Y_disordered))\n",
    "\n",
    "# pick random data points from ordered and disordered states \n",
    "# to create the training and test sets\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio)\n",
    "\n",
    "# full data set\n",
    "#X=np.concatenate((X_critical,X))\n",
    "#Y=np.concatenate((Y_critical,Y))\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "#print(X_critical.shape[0], 'critical samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining logistici regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    def __init__(self, eta, verbose):\n",
    "        #learing rate\n",
    "        self.eta = eta\n",
    "        #self.num_iter = num_iter\n",
    "        #true of false, printing loss function or not\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        \n",
    "       \n",
    "    def sigmoid_fun(self, z):\n",
    "        #sigmoid function\n",
    "        return 1/(1+np.exp(-z))\n",
    "    def loss_fun(self, h, y):\n",
    "        #loss function\n",
    "        return (-y*np.log(h) - (1-y) * np.log(1-h)).mean()\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        #forward step\n",
    "        z = np.dot(inputs, self.weights)\n",
    "        h = self.sigmoid_fun(z)\n",
    "        return h\n",
    "        \n",
    "    def train(self, X, y,num_iter,lambda1):\n",
    "        \n",
    "        # weights initialization\n",
    "        self.weights = np.zeros(X.shape[1]+1)\n",
    "        \n",
    "        #adding intercept\n",
    "        X=np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)       \n",
    "        \n",
    "        #stopping criteria\n",
    "        delta_loss=10000\n",
    "        loss_error=0.000003\n",
    "        old_loss=1000        \n",
    "        i=0\n",
    "        \n",
    "        while (delta_loss > loss_error) and (i < num_iter): \n",
    "            i+=1\n",
    "            h=self.forward(X)\n",
    "            \n",
    "            #dw = np.dot(X.T, (h - y)) / X.shape[0]\n",
    "            #regularization\n",
    "            reg_term=(lambda1/X.shape[0])*self.weights\n",
    "            reg_term[0]=0\n",
    "            dw = np.dot(X.T, (h - y)) / X.shape[0] + reg_term\n",
    "            #db = (np.sum(h - y)) / X.shape[0]\n",
    "            #dw[-1] = db\n",
    "            self.weights -= self.eta * dw            \n",
    "            h=self.forward(X)\n",
    "            \n",
    "            loss = self.loss_fun(h, y)\n",
    "            delta_loss=abs(loss-old_loss)\n",
    "            old_loss=loss\n",
    "            \n",
    "            if(self.verbose ==True and i % 10 == 0):\n",
    "                print(i)\n",
    "                print(f'loss: {loss} \\t')\n",
    "    def sgd_train(self, X, y,num_iter,lambda1):\n",
    "        # weights initialization\n",
    "        self.weights = np.zeros(X.shape[1]+1)\n",
    "        #adding intercept\n",
    "        X=np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        m=X.shape[0]\n",
    "        \n",
    "        #stopping criteria\n",
    "        delta_loss=10000\n",
    "        loss_error=0.001\n",
    "        old_loss=1000        \n",
    "        j=0\n",
    "        \n",
    "        while (delta_loss > loss_error) and (j < num_iter): \n",
    "            j+=1\n",
    "            for i in range(m):\n",
    "                index=np.random.randint(0,m)\n",
    "                X_i=X[index,:]\n",
    "                #print(X_i)\n",
    "                y_i=y[index]\n",
    "                #print(y_i)\n",
    "                h=self.forward(X_i)\n",
    "                \n",
    "                #regularization\n",
    "                reg_term=lambda1*self.weights\n",
    "                reg_term[0]=0\n",
    "                dw = np.dot(X_i.T, (h - y_i)) + reg_term\n",
    "                self.weights -= self.eta * dw            \n",
    "                h=self.forward(X_i)\n",
    "            \n",
    "                loss = self.loss_fun(h, y_i)\n",
    "                delta_loss=abs(loss-old_loss)\n",
    "                old_loss=loss\n",
    "                if(self.verbose ==True and j % 10 == 0):\n",
    "                    print(f'loss: {loss} \\t')\n",
    "    \n",
    "     \n",
    "             \n",
    "    def predict(self, X):\n",
    "        X=np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "    \n",
    "        output = self.forward(X)\n",
    "        return output.round()\n",
    "    \n",
    "    def mini_batches(self,X,y,mini_batches_size):\n",
    "        m = np.shape(X)[0]\n",
    "        mini_batches = []\n",
    "        \n",
    "        #shuffle X,y\n",
    "        perm = list(np.random.permutation(m))\n",
    "        shuffled_X=X[perm,:]\n",
    "        shuffled_y=y[perm]\n",
    "        \n",
    "        n_mini_batches= math.floor(m/mini_batches_size)\n",
    "        \n",
    "        for i in range(n_mini_batches):\n",
    "            mini_batch_X= shuffled_X[i*mini_batches_size:(i+1)*mini_batches_size,:]\n",
    "            mini_batch_y= shuffled_y[i*mini_batches_size:(i+1)*mini_batches_size]\n",
    "            mini_batch = (mini_batch_X,mini_batch_y)\n",
    "            mini_batches.append(mini_batch)\n",
    "            \n",
    "        #last mini batch\n",
    "        if m % mini_batches_size != 0:\n",
    "            mini_batch_X= shuffled_X[n_mini_batches*mini_batches_size:m,:]\n",
    "            mini_batch_y= shuffled_y[n_mini_batches*mini_batches_size:m]\n",
    "            mini_batch = (mini_batch_X,mini_batch_y)\n",
    "            mini_batches.append(mini_batch)\n",
    "            n_mini_batches += 1\n",
    "        #mini_batches[0,0,:]== all X's from the a mini_batche 1\n",
    "        return mini_batches,n_mini_batches\n",
    "    \n",
    "    def sgd_mini_batch_train(self, X, y,num_iter,mini_batches_size,lambda1):\n",
    "        # weights initialization\n",
    "        self.weights = np.zeros(X.shape[1]+1)\n",
    "        #adding intercept\n",
    "        X=np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        \n",
    "        batches,n_mini_batches=self.mini_batches(X,y,mini_batches_size)\n",
    "        \n",
    "        delta_loss=10000\n",
    "        loss_error=0.0001\n",
    "        old_loss=1000        \n",
    "        j=0\n",
    "        \n",
    "        while (delta_loss > loss_error) and (j < num_iter): \n",
    "            j+=1            \n",
    "            for i in range(n_mini_batches):\n",
    "                index=np.random.randint(0,n_mini_batches)\n",
    "                #0==X\n",
    "                #1==y\n",
    "                X_i=batches[index][0][:]\n",
    "                #print(X_i)\n",
    "                y_i=batches[index][1][:]\n",
    "                #print(y_i)\n",
    "                h=self.forward(X_i)\n",
    "                \n",
    "                reg_term=(lambda1/X.shape[0])*self.weights\n",
    "                reg_term[0]=0\n",
    "                dw = np.dot(X_i.T, (h - y_i)) / mini_batches_size + reg_term\n",
    "                self.weights -= self.eta * dw            \n",
    "                h=self.forward(X_i)\n",
    "            \n",
    "                loss = self.loss_fun(h, y_i)\n",
    "                delta_loss=abs(loss-old_loss)\n",
    "                old_loss=loss\n",
    "            \n",
    "            if(self.verbose ==True and j % 10 == 0):\n",
    "                print(f'loss: {loss} \\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds_CV(dataset, nfolds):\n",
    "    # Split a dataset into k folds\n",
    "    splitedDataset = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / nfolds)\n",
    "    for i in range(nfolds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        splitedDataset.append(fold)\n",
    "    return splitedDataset\n",
    "\n",
    "def trainSetindex(indeces,testSetindex):\n",
    "    #given indeces of the test set + valid set, find the indeces of the train set\n",
    "    size=np.size(indeces)\n",
    "    mask = np.ones(size, dtype=bool)\n",
    "    mask[testSetindex] = False\n",
    "    return indeces[mask]\n",
    "\n",
    "\n",
    "indeces=np.linspace(0,np.shape(X)[0]-1,np.shape(X)[0])\n",
    "indeces=indeces.astype(int)\n",
    "np.random.shuffle(indeces)\n",
    "\n",
    "numberOfFolds=5\n",
    "folds = k_folds_CV(indeces, numberOfFolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing results for the ordinary gradient solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define regularisation parameter\n",
    "lmbdas=np.logspace(-5,5,11)\n",
    "\n",
    "# preallocate data\n",
    "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "\n",
    "\n",
    "# loop over regularisation strength\n",
    "for j,lmbda in enumerate(lmbdas):\n",
    "\n",
    "    model1 = LogReg(eta=0.001,verbose=False)\n",
    "    model1.train(X_train, Y_train, num_iter=1000,lambda1=lmbda)\n",
    "    preds = model1.predict(X_train)\n",
    "\n",
    "    train_accuracy[j]=(preds == Y_train).mean()\n",
    "    \n",
    "    pred1 = model1.predict(X_test)\n",
    "    test_accuracy[j]=(pred1 == Y_test).mean()  \n",
    "    print('Linear: %0.4f, %0.4f' %(train_accuracy[j],test_accuracy[j]) )\n",
    "    print('finished computing %i/11 iterations' %(j+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing results for the  SGD gradient solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define regularisation parameter\n",
    "lmbdas=np.logspace(-5,5,11)\n",
    "\n",
    "# preallocate data\n",
    "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "\n",
    "\n",
    "# loop over regularisation strength\n",
    "for j,lmbda in enumerate(lmbdas):\n",
    "\n",
    "    model = LogReg(eta=0.001,verbose=False)\n",
    "    model.sgd_mini_batch_train(X_train, Y_train, num_iter=100,mini_batches_size=65,lambda1=lmbda)\n",
    "    preds = model.predict(X_train)\n",
    "    train_accuracy_SGD[j]=(preds == Y_train).mean()    \n",
    "    pred1 = model.predict(X_test)\n",
    "    test_accuracy_SGD[j]=(pred1 == Y_test).mean()\n",
    "    print('SGD: %0.4f, %0.4f' %(train_accuracy_SGD[j],test_accuracy_SGD[j]) )\n",
    "    print('finished computing %i/11 iterations' %(j+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot accuracy against regularisation strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy against regularisation strength\n",
    "fig=plt.figure()\n",
    "plt.semilogx(lmbdas,train_accuracy,'*-b',label='classLR train')\n",
    "plt.semilogx(lmbdas,test_accuracy,'*-r',label='classLR test')\n",
    "\n",
    "\n",
    "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='SGD train')\n",
    "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label='SGD test')\n",
    "\n",
    "plt.xlabel('$\\\\lambda$')\n",
    "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('LRaccuracy12.png',DPI=(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part e) Classifying the Ising model phase using neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot a few Ising states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# set colourbar map\n",
    "cmap_args=dict(cmap='plasma_r')\n",
    "\n",
    "# plot states\n",
    "fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "axarr[0].imshow(X_ordered[20001].reshape(L,L),**cmap_args)\n",
    "axarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\n",
    "axarr[0].tick_params(labelsize=16)\n",
    "\n",
    "axarr[1].imshow(X_critical[10001].reshape(L,L),**cmap_args)\n",
    "axarr[1].set_title('$\\\\mathrm{critical\\\\ region}$',fontsize=16)\n",
    "axarr[1].tick_params(labelsize=16)\n",
    "\n",
    "im=axarr[2].imshow(X_disordered[50001].reshape(L,L),**cmap_args)\n",
    "axarr[2].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\n",
    "axarr[2].tick_params(labelsize=16)\n",
    "\n",
    "fig.subplots_adjust(right=2.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the one-hot encoded target vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cross = X_train\n",
    "\n",
    "temp = np.zeros((len(Y_train), 2))\n",
    "order = np.zeros(2)\n",
    "order[0] = 1\n",
    "disorder = np.zeros(2)\n",
    "disorder[1] = 1\n",
    "for i in range(len(Y_train)):\n",
    "    if Y_train[i] == 1:\n",
    "        temp[i] = order\n",
    "    if Y_train[i] == 0:\n",
    "        temp[i] = disorder\n",
    "\n",
    "Y_train = temp\n",
    "temp = np.zeros((len(Y_test), 2))\n",
    "for i in range(len(Y_test)):\n",
    "    if Y_test[i] == 1:\n",
    "        temp[i] = order\n",
    "    if Y_test[i] == 0:\n",
    "        temp[i] = disorder\n",
    "Y_test = temp\n",
    "\n",
    "Y_cri = np.empty((len(Y_critical),2))\n",
    "for i in range(len(Y_cri)):\n",
    "    if Y_critical[i] == 1:\n",
    "        Y_cri[i] = order\n",
    "    else:\n",
    "        Y_cri[i] = disorder\n",
    "\n",
    "Y_cross = Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 16250\n",
    "\n",
    "X_valid = X_train[-val_size:]\n",
    "Y_valid = Y_train[-val_size:]\n",
    "\n",
    "X_train = X_train[:-val_size]\n",
    "Y_train = Y_train[:-val_size]\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_valid.shape[0], 'validation samples')\n",
    "print(X_critical.shape[0], 'critical samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation for the Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(data, targets, test, test_targets, nhidden=5, k=10):\n",
    "\n",
    "    data_sp = np.split(data, k)\n",
    "    size = int(len(data)/k)\n",
    "    targets_sp = np.split(targets, k)\n",
    "    errs = np.empty(k)\n",
    "    accs = np.empty(k)\n",
    "    \n",
    "    for i in range(k):\n",
    "        t_ind = np.arange(len(data))\n",
    "        t_ind = np.delete(t_ind, t_ind[i*size:(i+1)*size])\n",
    "        print(t_ind)\n",
    "        valid = data_sp[i]\n",
    "        valid_targets = targets_sp[i]\n",
    "        train = data[t_ind]\n",
    "        train_targets = targets[t_ind]\n",
    "        print(train.shape)\n",
    "        print(train_targets.shape)\n",
    "\n",
    "        net = mlp.mlp(train, train_targets, nhidden)\n",
    "\n",
    "        val_err, val_acc, tr_err, tr_acc = net.earlystopping(train, train_targets, valid, valid_targets, 50)\n",
    "\n",
    "        plt.plot(val_err)\n",
    "        plt.figure()\n",
    "        plt.plot(val_acc)\n",
    "        plt.figure()\n",
    "        plt.plot(tr_err)\n",
    "        plt.figure()\n",
    "        plt.plot(tr_acc)\n",
    "        plt.show()\n",
    "\n",
    "        errs[i], accs[i] = net.test(test, test_targets)\n",
    "\n",
    "        print(\"Error of %d fold is: \" % (i+1), errs[i])\n",
    "        print(\"Accuracy of %d fold is: \" % (i+1), accs[i])\n",
    "\n",
    "    print(\"Mean error is: \", np.mean(errs))\n",
    "    print(\"Standard deviation of error is: \", np.std(errs))\n",
    "    print(\"Mean accuracy is: \", np.mean(accs))\n",
    "    print(\"Standard deviation of accuracy is: \", np.std(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Neural Network and plotting its learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = mlp.mlp(X_train, Y_train, 5)\n",
    "\n",
    "val_err, val_acc, tr_err, tr_acc = net.earlystopping(X_train, Y_train, X_valid, Y_valid, 100)\n",
    "\n",
    "plt.plot(val_err)\n",
    "plt.plot(tr_err)\n",
    "plt.figure()\n",
    "plt.plot(val_acc)\n",
    "plt.plot(tr_acc)\n",
    "print(\"################################\")\n",
    "print(\"Test Results\")\n",
    "net.test(X_test, Y_test)\n",
    "print(\"################################\")\n",
    "print(\"Critical Results\")\n",
    "net.test(X_critical, Y_cri)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(X_cross, Y_cross, X_test, Y_test, nhidden=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining plot fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x,y,data):\n",
    "\n",
    "    # plot results\n",
    "    fontsize=16\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(data, interpolation='nearest', vmin=0, vmax=1)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # put text on matrix elements\n",
    "    for i, x_val in enumerate(np.arange(len(x))):\n",
    "        for j, y_val in enumerate(np.arange(len(y))):\n",
    "            c = \"${0:.1f}\\\\%$\".format( 100*data[j,i])  \n",
    "            ax.text(x_val, y_val, c, va='center', ha='center')\n",
    "\n",
    "    # convert axis vaues to to string labels\n",
    "    x=[str(i) for i in x]\n",
    "    y=[str(i) for i in y]\n",
    "\n",
    "\n",
    "    ax.set_xticklabels(['']+x)\n",
    "    ax.set_yticklabels(['']+y)\n",
    "\n",
    "    ax.set_xlabel('$\\\\mathrm{learning\\\\ rate}$',fontsize=fontsize)\n",
    "    ax.set_ylabel('$\\\\mathrm{hidden\\\\ neurons}$',fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_data_lam(x,y,data):\n",
    "\n",
    "    # plot results\n",
    "    fontsize=16\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(data, interpolation='nearest', vmin=0, vmax=1)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # put text on matrix elements\n",
    "    for i, x_val in enumerate(np.arange(len(x))):\n",
    "        for j, y_val in enumerate(np.arange(len(y))):\n",
    "            c = \"${0:.1f}\\\\%$\".format( 100*data[j,i])  \n",
    "            ax.text(x_val, y_val, c, va='center', ha='center', fontsize=16)\n",
    "\n",
    "    # convert axis vaues to to string labels\n",
    "    x=[str(i) for i in x]\n",
    "    y=[str(i) for i in y]\n",
    "\n",
    "\n",
    "    ax.set_xticklabels(['']+x)\n",
    "    ax.set_yticklabels(['']+y)\n",
    "\n",
    "    ax.set_xlabel('$\\\\mathrm{learning\\\\ rate}$',fontsize=fontsize)\n",
    "    ax.set_ylabel('$\\\\mathrm{ReLU\\\\ Scaler}$',fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different parameters for hidden nodes and learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etas = [1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 0.1]\n",
    "nodes = [1, 5, 10, 20]\n",
    "\n",
    "ltr_acc = np.empty((len(nodes),len(etas)))\n",
    "ltr_err = np.empty((len(nodes),len(etas)))\n",
    "test_acc = np.empty((len(nodes),len(etas)))\n",
    "test_err = np.empty((len(nodes),len(etas)))\n",
    "cri_acc = np.empty((len(nodes),len(etas)))\n",
    "cri_err = np.empty((len(nodes),len(etas)))\n",
    "\n",
    "for i,num_nodes in enumerate(nodes):\n",
    "    for j, eta in enumerate(etas):\n",
    "        net = mlp.mlp(X_train, Y_train, num_nodes, eta=eta)\n",
    "\n",
    "        val_err, val_acc, tr_err, tr_acc = net.earlystopping(X_train, Y_train, X_valid, Y_valid, 50)\n",
    "\n",
    "        ltr_acc[i][j] = tr_acc[-1]\n",
    "        ltr_err[i][j] = tr_err[-1]\n",
    "\n",
    "        print(\"################################\")\n",
    "        print(\"Test Results\")\n",
    "        test_err[i][j], test_acc[i][j] = net.test(X_test, Y_test)\n",
    "        print(\"################################\")\n",
    "        print(\"Critical Results\")\n",
    "        cri_err[i][j], cri_acc[i][j] = net.test(X_critical, Y_cri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting eta vs. number of hidden nodes, results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(etas, nodes, ltr_acc)\n",
    "plot_data(etas, nodes, test_acc)\n",
    "plot_data(etas, nodes, cri_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for the lambda (ReLU scaler) versus eta parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lams = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "ltr_acc = np.empty((len(lams),len(etas)))\n",
    "ltr_err = np.empty((len(lams),len(etas)))\n",
    "test_acc = np.empty((len(lams),len(etas)))\n",
    "test_err = np.empty((len(lams),len(etas)))\n",
    "cri_acc = np.empty((len(lams),len(etas)))\n",
    "cri_err = np.empty((len(lams),len(etas)))\n",
    "\n",
    "for i,lam in enumerate(lams):\n",
    "    for j, eta in enumerate(etas):\n",
    "        net = mlp.mlp(X_train, Y_train, 10, eta=eta, lam=lam)\n",
    "\n",
    "        val_err, val_acc, tr_err, tr_acc = net.earlystopping(X_train, Y_train, X_valid, Y_valid, 100)\n",
    "\n",
    "        ltr_acc[i][j] = tr_acc[-1]\n",
    "        ltr_err[i][j] = tr_err[-1]\n",
    "\n",
    "        print(\"################################\")\n",
    "        print(\"Test Results\")\n",
    "        test_err[i][j], test_acc[i][j] = net.test(X_test, Y_test)\n",
    "        print(\"################################\")\n",
    "        print(\"Critical Results\")\n",
    "        cri_err[i][j], cri_acc[i][j] = net.test(X_critical, Y_cri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting lambda (ReLU Scaler) vs. eta for 10 hidden nodes, results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_lam(etas, lams, ltr_acc)\n",
    "plot_data_lam(etas, lams, test_acc)\n",
    "plot_data_lam(etas, lams, cri_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
